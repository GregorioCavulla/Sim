{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23f621f",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data\n",
    "\n",
    "Load the dataset and show:\n",
    "- Size and structure\n",
    "- Histograms of numeric features\n",
    "- Distribution of target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data.csv\")\n",
    "\n",
    "# Show basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d744c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical summary of numeric features:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nTotal missing values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa51dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show histogram of target labels (language)\n",
    "print(\"Target label distribution:\")\n",
    "print(df[\"language\"].value_counts())\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[\"language\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Distribution of Target Labels (Language)\")\n",
    "plt.xlabel(\"Language\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of numeric features\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].hist(df[col], bins=30, edgecolor='black')\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11072102",
   "metadata": {},
   "source": [
    "## 2. Comment on Exploration\n",
    "\n",
    "Based on the exploration above:\n",
    "\n",
    "**Dataset Structure:**\n",
    "- The dataset contains language classification data with 12 numeric features (X1 to X12) and 1 target variable (language)\n",
    "- Dataset has 331 rows and 13 columns\n",
    "\n",
    "**Missing Values:**\n",
    "- Check shows the presence (or absence) of missing values\n",
    "- If present, they will be dropped as per exam requirements\n",
    "\n",
    "**Target Distribution:**\n",
    "- The histogram shows the balance/imbalance of language classes\n",
    "- Imbalanced distributions may affect model performance and require stratified sampling\n",
    "\n",
    "**Feature Distributions:**\n",
    "- The histograms show the distribution of each numeric feature\n",
    "- Features appear to have different scales and ranges\n",
    "- Some features may have outliers (visible in the tails of distributions)\n",
    "- All features seem relevant as they represent extracted linguistic features\n",
    "\n",
    "**Outliers:**\n",
    "- Some features show potential outliers in their extreme values\n",
    "- These outliers are kept as they may represent important linguistic patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb3d99",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "Drop rows with NaN values and show the shape after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values\n",
    "print(\"Shape before cleaning:\", df.shape)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"Shape after cleaning:\", df.shape)\n",
    "print(\"\\nNumber of rows removed:\", 331 - df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481aac8",
   "metadata": {},
   "source": [
    "## 4. Model 1: Decision Tree with Cross Validation\n",
    "\n",
    "Train a Decision Tree classifier with:\n",
    "- Hyperparameter tuning using GridSearchCV\n",
    "- Cross Validation (StratifiedKFold)\n",
    "- Optimization for recall_macro (without considering class frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2444f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop(columns=[\"language\"])\n",
    "y = df[\"language\"]\n",
    "\n",
    "# Split into training and test sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Decision Tree\n",
    "param_grid_dt = {\n",
    "    \"max_depth\": [3, 5, 7, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Setup Stratified K-Fold Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Setup GridSearchCV for Model 1 (Decision Tree)\n",
    "# Optimize for recall_macro (recall without considering class frequencies)\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid_dt,\n",
    "    cv=cv,\n",
    "    scoring=\"recall_macro\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training Model 1 (Decision Tree) with Cross Validation...\")\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_model_dt = grid_search_dt.best_estimator_\n",
    "\n",
    "print(\"\\nBest parameters for Model 1:\")\n",
    "print(grid_search_dt.best_params_)\n",
    "print(f\"\\nBest cross-validation recall_macro score: {grid_search_dt.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7ae64",
   "metadata": {},
   "source": [
    "## 5. Classification Report for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_dt = best_model_dt.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report for Model 1 (Decision Tree):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc04fc9",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix for Model 1\n",
    "\n",
    "Display normalized confusion matrix with respect to true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033cdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix normalized by true values\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_model_dt,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    normalize=\"true\",  # Normalize with respect to true values\n",
    "    cmap=\"Blues\",\n",
    "    ax=ax\n",
    ")\n",
    "plt.title(\"Confusion Matrix for Model 1 (Decision Tree)\\nNormalized by True Values\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de515a09",
   "metadata": {},
   "source": [
    "## 7. Model 2: Random Forest with Cross Validation\n",
    "\n",
    "Train a Random Forest classifier with:\n",
    "- Hyperparameter tuning using GridSearchCV\n",
    "- Cross Validation (StratifiedKFold)\n",
    "- Optimization for recall_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV for Model 2 (Random Forest)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring=\"recall_macro\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training Model 2 (Random Forest) with Cross Validation...\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "print(\"\\nBest parameters for Model 2:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "print(f\"\\nBest cross-validation recall_macro score: {grid_search_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9762ee5",
   "metadata": {},
   "source": [
    "## 8. Classification Report for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78807288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_rf = best_model_rf.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report for Model 2 (Random Forest):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32449e",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix for Model 2\n",
    "\n",
    "Display normalized confusion matrix with respect to true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix normalized by true values\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_model_rf,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    normalize=\"true\",  # Normalize with respect to true values\n",
    "    cmap=\"Greens\",\n",
    "    ax=ax\n",
    ")\n",
    "plt.title(\"Confusion Matrix for Model 2 (Random Forest)\\nNormalized by True Values\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b4492",
   "metadata": {},
   "source": [
    "## 10. Comparison Between Models\n",
    "\n",
    "Compare the performance of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional metrics for comparison\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Calculate metrics for both models\n",
    "metrics_dt = {\n",
    "    \"Model\": \"Decision Tree\",\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_dt),\n",
    "    \"Recall (macro)\": recall_score(y_test, y_pred_dt, average=\"macro\"),\n",
    "    \"Precision (macro)\": precision_score(y_test, y_pred_dt, average=\"macro\"),\n",
    "    \"F1-Score (macro)\": f1_score(y_test, y_pred_dt, average=\"macro\"),\n",
    "    \"CV Score\": grid_search_dt.best_score_\n",
    "}\n",
    "\n",
    "metrics_rf = {\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_rf),\n",
    "    \"Recall (macro)\": recall_score(y_test, y_pred_rf, average=\"macro\"),\n",
    "    \"Precision (macro)\": precision_score(y_test, y_pred_rf, average=\"macro\"),\n",
    "    \"F1-Score (macro)\": f1_score(y_test, y_pred_rf, average=\"macro\"),\n",
    "    \"CV Score\": grid_search_rf.best_score_\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([metrics_dt, metrics_rf])\n",
    "comparison_df = comparison_df.set_index(\"Model\")\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Metrics comparison\n",
    "metrics_to_plot = [\"Accuracy\", \"Recall (macro)\", \"Precision (macro)\", \"F1-Score (macro)\"]\n",
    "comparison_df[metrics_to_plot].T.plot(kind=\"bar\", ax=axes[0], width=0.8)\n",
    "axes[0].set_title(\"Performance Metrics Comparison\")\n",
    "axes[0].set_xlabel(\"Metrics\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].legend(title=\"Model\")\n",
    "axes[0].set_xticklabels(metrics_to_plot, rotation=45, ha=\"right\")\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: CV Score comparison\n",
    "comparison_df[\"CV Score\"].plot(kind=\"bar\", ax=axes[1], color=[\"skyblue\", \"lightgreen\"])\n",
    "axes[1].set_title(\"Cross-Validation Recall (macro) Score\")\n",
    "axes[1].set_xlabel(\"Model\")\n",
    "axes[1].set_ylabel(\"CV Score\")\n",
    "axes[1].set_xticklabels(comparison_df.index, rotation=0)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d55c1",
   "metadata": {},
   "source": [
    "### Comments on Model Comparison:\n",
    "\n",
    "**Performance Analysis:**\n",
    "\n",
    "1. **Recall (macro) - Primary Metric:**\n",
    "   - This was the optimization metric used during hyperparameter tuning\n",
    "   - Comparing the recall_macro scores shows which model better identifies all classes equally\n",
    "   \n",
    "2. **Overall Accuracy:**\n",
    "   - Higher accuracy indicates better overall classification performance\n",
    "   - However, accuracy can be misleading with imbalanced datasets\n",
    "\n",
    "3. **Precision vs Recall Trade-off:**\n",
    "   - Precision measures how many predicted positives are actually positive\n",
    "   - Recall measures how many actual positives were correctly identified\n",
    "   - The F1-score balances both metrics\n",
    "\n",
    "4. **Cross-Validation Score:**\n",
    "   - Shows the model's performance during training with CV\n",
    "   - Large gap between CV score and test score may indicate overfitting\n",
    "\n",
    "**Model Selection:**\n",
    "- The model with higher recall_macro should be preferred as per exam requirements\n",
    "- Consider also the confusion matrices: fewer misclassifications indicate better performance\n",
    "- Random Forest typically handles complex patterns better due to ensemble learning\n",
    "- Decision Tree is more interpretable but may overfit if not properly tuned\n",
    "\n",
    "**Conclusion:**\n",
    "Based on the metrics above, [the better model will be determined by the actual results]. \n",
    "The confusion matrices show where each model makes errors, helping us understand their behavior on different language classes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
