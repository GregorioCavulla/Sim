{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365e4454",
   "metadata": {},
   "source": [
    "# üéØ EXAM TYPE IDENTIFICATION GUIDE\n",
    "\n",
    "**First, read the exam text carefully and identify keywords to determine the type:**\n",
    "\n",
    "## üìä Classification Keywords:\n",
    "- `classification_report`, `confusion_matrix`, `precision`, `recall`, `accuracy`, `f1-score`\n",
    "- Terms: \"classify\", \"predict class\", \"categories\", \"labels\"\n",
    "- Metric requirements: \"maximize recall\", \"accuracy score\"\n",
    "\n",
    "## üîç Clustering Keywords:\n",
    "- `silhouette_score`, `K-means`, `DBSCAN`, `AgglomerativeClustering`\n",
    "- Terms: \"unsupervised\", \"group similar\", \"gold standard\", \"remap clusters\"\n",
    "- Tasks: \"find optimal k\", \"compare with ground truth\"\n",
    "\n",
    "## üìà Regression Keywords:\n",
    "- `RMSE`, `MSE`, `MAE`, `R2`, \"mean squared error\", \"predict continuous value\"\n",
    "- Terms: \"predict\", \"estimate\", \"forecast\" (with numeric target)\n",
    "- Tasks: \"minimize RMSE\", \"feature selection by correlation\"\n",
    "\n",
    "## üõí Association Rules Keywords:\n",
    "- `apriori`, `support`, `confidence`, `lift`, \"frequent itemsets\", \"basket analysis\"\n",
    "- Terms: \"transactional data\", \"market basket\", \"recommendations\"\n",
    "- Tasks: \"find rules with lift > X\", \"optimal support\"\n",
    "\n",
    "**üí° Once identified, jump to the relevant sections marked with üìå indicators!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd452965",
   "metadata": {},
   "source": [
    "## Section 1: Import All Required Libraries\n",
    "\n",
    "**üìå Use for: ALL exam types**\n",
    "\n",
    "Import everything you might need for any exam type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eca6058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIVERSAL IMPORTS - USE FOR ALL EXAM TYPES\n",
    "# ============================================\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Train/Test split\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif, mutual_info_regression\n",
    "\n",
    "# ============================================\n",
    "# CLASSIFICATION\n",
    "# ============================================\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# REGRESSION\n",
    "# ============================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ============================================\n",
    "# CLUSTERING\n",
    "# ============================================\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# ============================================\n",
    "# ASSOCIATION RULES\n",
    "# ============================================\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "\n",
    "url = \"data.csv\" # CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfb175",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Initial Exploration\n",
    "\n",
    "**üìå Use for: ALL exam types**\n",
    "\n",
    "**Universal pattern for all exam types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331bee3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# LOAD DATA - ADJUST FILE NAME AND TYPE\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# For CSV files\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# For Excel files (Association Rules)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# df = pd.read_excel(url)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Show basic information\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/venvs/mldm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venvs/mldm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/venvs/mldm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venvs/mldm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/venvs/mldm/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD DATA - ADJUST FILE NAME AND TYPE\n",
    "# ============================================\n",
    "\n",
    "# For CSV files\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# For Excel files (Association Rules)\n",
    "# df = pd.read_excel(url)\n",
    "\n",
    "# Show basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATA EXPLORATION\n",
    "# ============================================\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHECK FOR MISSING VALUES\n",
    "# ============================================\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dcd5f",
   "metadata": {},
   "source": [
    "## Section 3: Data Cleaning\n",
    "\n",
    "**üìå Use for: ALL exam types**\n",
    "\n",
    "**Common cleaning operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CLEAN STRING COLUMNS (if any)\n",
    "# ============================================\n",
    "\n",
    "# Remove leading/trailing spaces from string columns\n",
    "# string_cols = ['Description', 'Category']  # Adjust based on your data\n",
    "# for col in string_cols:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = df[col].str.strip()\n",
    "\n",
    "print(\"String columns cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca08fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HANDLE MISSING VALUES\n",
    "# ============================================\n",
    "\n",
    "print(f\"Rows before cleaning: {df.shape[0]}\")\n",
    "\n",
    "# Option 1: Drop rows with NaN (most common in exams)\n",
    "df = df.dropna()\n",
    "\n",
    "# Option 2: Fill with mean (for numeric columns)\n",
    "# df['numeric_col'] = df['numeric_col'].fillna(df['numeric_col'].mean())\n",
    "\n",
    "# Option 3: Fill with mode or 'Unknown' (for categorical)\n",
    "# df['category_col'] = df['category_col'].fillna('Unknown')\n",
    "\n",
    "print(f\"Rows after cleaning: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90a8d1",
   "metadata": {},
   "source": [
    "## Section 3.5: Advanced Preprocessing with Pipeline and ColumnTransformer\n",
    "\n",
    "**üìå Use for: Classification, Regression**\n",
    "\n",
    "**Use when you have mixed column types or need sophisticated preprocessing pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IDENTIFY COLUMN TYPES\n",
    "# ============================================\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove target if present\n",
    "if 'target' in numeric_features:\n",
    "    numeric_features.remove('target')\n",
    "if 'target' in categorical_features:\n",
    "    categorical_features.remove('target')\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff99e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE PREPROCESSING PIPELINE\n",
    "# ============================================\n",
    "\n",
    "# Numeric transformer: Impute missing values + Scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # or 'median'\n",
    "    ('scaler', StandardScaler())  # or MinMaxScaler()\n",
    "])\n",
    "\n",
    "# Categorical transformer: Impute + OneHotEncode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"‚úÖ Preprocessor created\")\n",
    "print(f\"   - Numeric pipeline: {len(numeric_features)} features\")\n",
    "print(f\"   - Categorical pipeline: {len(categorical_features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULL PIPELINE: PREPROCESSING + MODEL\n",
    "# ============================================\n",
    "\n",
    "# Example: Classification pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Assume X, y are defined from previous sections\n",
    "# X = df.drop(columns=['target'])\n",
    "# y = df['target']\n",
    "\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Fit pipeline\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Pipeline accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n‚úÖ Pipeline approach allows clean separation of preprocessing and modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357169f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ORDINAL ENCODING EXAMPLE\n",
    "# ============================================\n",
    "\n",
    "# For categorical features with natural order (e.g., 'Low', 'Medium', 'High')\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Example:\n",
    "# ordinal_features = ['education_level']  # Has order: 'High School' < 'Bachelor' < 'Master'\n",
    "# ordinal_categories = [['High School', 'Bachelor', 'Master', 'PhD']]\n",
    "\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "# ])\n",
    "\n",
    "# Then add to ColumnTransformer:\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('ord', ordinal_transformer, ordinal_features),\n",
    "#         ('cat', categorical_transformer, other_categorical_features)\n",
    "#     ])\n",
    "\n",
    "print(\"üí° Use OrdinalEncoder when categorical features have meaningful order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0629fd",
   "metadata": {},
   "source": [
    "## Section 4: Target Variable Analysis\n",
    "\n",
    "**üìå Use for: Classification, Regression**\n",
    "\n",
    "**For Classification and Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IDENTIFY TARGET VARIABLE\n",
    "# ============================================\n",
    "\n",
    "# Adjust 'target' to your actual target column name\n",
    "# Common names: 'language', 'class', 'label', 'y', 'target'\n",
    "\n",
    "target_col = 'target'  # CHANGE THIS\n",
    "\n",
    "if target_col in df.columns:\n",
    "    # For CLASSIFICATION: show distribution\n",
    "    print(\"Target distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df[target_col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {target_col}')\n",
    "    plt.xlabel(target_col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # For REGRESSION: show statistics\n",
    "    if df[target_col].dtype in ['int64', 'float64']:\n",
    "        print(f\"\\nTarget statistics:\")\n",
    "        print(df[target_col].describe())\n",
    "else:\n",
    "    print(f\"Target column '{target_col}' not found. Available columns:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191a0c8",
   "metadata": {},
   "source": [
    "## Section 5: Feature Visualization\n",
    "\n",
    "**üìå Use for: Classification, Regression, Clustering**\n",
    "\n",
    "**Histograms and distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dee061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HISTOGRAMS OF NUMERIC FEATURES\n",
    "# ============================================\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove target if present\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = min(4, len(numeric_cols))\n",
    "    n_rows = int(np.ceil(len(numeric_cols) / n_cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.ravel() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        axes[idx].hist(df[col], bins=30, edgecolor='black')\n",
    "        axes[idx].set_title(f'Distribution of {col}')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BOXPLOT FOR OUTLIER DETECTION\n",
    "# ============================================\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    df[numeric_cols].boxplot()\n",
    "    plt.title('Boxplot of Numeric Features')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRELATION MATRIX (for Regression/Classification)\n",
    "# ============================================\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[numeric_cols + [target_col]].corr() if target_col in df.columns else df[numeric_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlation with target\n",
    "    if target_col in df.columns:\n",
    "        print(\"\\nCorrelation with target:\")\n",
    "        print(correlation_matrix[target_col].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b21151",
   "metadata": {},
   "source": [
    "## Section 6: CLASSIFICATION - Complete Workflow\n",
    "\n",
    "**üìå Use for: Classification only**\n",
    "\n",
    "**Use this section for classification problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARE DATA FOR CLASSIFICATION\n",
    "# ============================================\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Train/Test Split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b145a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODEL 1: DECISION TREE WITH GRID SEARCH\n",
    "# ============================================\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Setup Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "# ADJUST SCORING: 'recall_macro', 'f1_macro', 'accuracy'\n",
    "grid_dt = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid_dt,\n",
    "    cv=cv,\n",
    "    scoring='recall_macro',  # CHANGE IF NEEDED\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "print(\"Training Decision Tree with GridSearchCV...\")\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt = grid_dt.best_estimator_\n",
    "print(f\"\\nBest parameters: {grid_dt.best_params_}\")\n",
    "print(f\"Best CV score: {grid_dt.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATE MODEL 1\n",
    "# ============================================\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report - Decision Tree:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_dt, X_test, y_test, normalize='true', cmap='Blues'\n",
    ")\n",
    "plt.title('Confusion Matrix - Decision Tree (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f01e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODEL 2: RANDOM FOREST WITH GRID SEARCH\n",
    "# ============================================\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring='recall_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest with GridSearchCV...\")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(f\"\\nBest parameters: {grid_rf.best_params_}\")\n",
    "print(f\"Best CV score: {grid_rf.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "print(\"\\nClassification Report - Random Forest:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_rf, X_test, y_test, normalize='true', cmap='Greens'\n",
    ")\n",
    "plt.title('Confusion Matrix - Random Forest (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE CLASSIFICATION MODELS\n",
    "# ============================================\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_dt),\n",
    "        accuracy_score(y_test, y_pred_rf)\n",
    "    ],\n",
    "    'Recall (macro)': [\n",
    "        recall_score(y_test, y_pred_dt, average='macro'),\n",
    "        recall_score(y_test, y_pred_rf, average='macro')\n",
    "    ],\n",
    "    'Precision (macro)': [\n",
    "        precision_score(y_test, y_pred_dt, average='macro'),\n",
    "        precision_score(y_test, y_pred_rf, average='macro')\n",
    "    ],\n",
    "    'F1-Score (macro)': [\n",
    "        f1_score(y_test, y_pred_dt, average='macro'),\n",
    "        f1_score(y_test, y_pred_rf, average='macro')\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25198ca",
   "metadata": {},
   "source": [
    "## Section 6.5: Feature Selection for Classification\n",
    "\n",
    "**üìå Use for: Classification only**\n",
    "\n",
    "**Reduce dimensionality by selecting most informative features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01726e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELECTKBEST WITH MUTUAL INFORMATION\n",
    "# ============================================\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are already defined from Section 6\n",
    "\n",
    "# Choose number of top features to select\n",
    "k = 10  # ADJUST based on requirements\n",
    "\n",
    "# Create selector with mutual information\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform test data (use same features)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Selected features: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Get selected feature names (if X is DataFrame)\n",
    "if hasattr(X_train, 'columns'):\n",
    "    selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "    print(f\"\\nSelected features: {selected_features}\")\n",
    "    \n",
    "    # Show feature scores\n",
    "    scores = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Score': selector.scores_\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    print(\"\\nFeature scores:\")\n",
    "    print(scores.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d57818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAIN MODEL WITH SELECTED FEATURES\n",
    "# ============================================\n",
    "\n",
    "# Train classifier on reduced feature set\n",
    "clf_selected = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "clf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_selected = clf_selected.predict(X_test_selected)\n",
    "\n",
    "print(\"\\nClassification Report (Selected Features):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_selected))\n",
    "\n",
    "# Compare accuracy\n",
    "acc_full = accuracy_score(y_test, y_pred_dt)  # From Section 6\n",
    "acc_selected = accuracy_score(y_test, y_pred_selected)\n",
    "\n",
    "print(f\"\\nAccuracy comparison:\")\n",
    "print(f\"  Full features ({X_train.shape[1]}): {acc_full:.4f}\")\n",
    "print(f\"  Selected features ({k}): {acc_selected:.4f}\")\n",
    "print(f\"  Difference: {acc_selected - acc_full:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8744f0",
   "metadata": {},
   "source": [
    "## Section 7: CLUSTERING - Pairplot and Exploration\n",
    "\n",
    "**üìå Use for: Clustering only**\n",
    "\n",
    "**Use this section for clustering problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81732ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARE DATA FOR CLUSTERING\n",
    "# ============================================\n",
    "\n",
    "# Separate X (all columns but last) and y (last column - gold standard)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Gold standard shape: {y.shape}\")\n",
    "print(f\"Number of clusters in gold standard: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d183c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PAIRPLOT WITH GOLD STANDARD\n",
    "# ============================================\n",
    "\n",
    "X_with_labels = X.copy()\n",
    "X_with_labels['Gold_Standard'] = y\n",
    "\n",
    "sns.pairplot(X_with_labels, hue='Gold_Standard', palette='Set1', diag_kind='kde')\n",
    "plt.suptitle('Pairplot (colored by Gold Standard)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36ebd3",
   "metadata": {},
   "source": [
    "## Section 8: CLUSTERING - K-Means with Silhouette\n",
    "\n",
    "**üìå Use for: Clustering only**\n",
    "\n",
    "**Find optimal clusters and compare with gold standard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SILHOUETTE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "k_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k={k}: Silhouette = {score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linewidth=2)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_range)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose k (can be from gold standard or visual inspection)\n",
    "chosen_k = y.nunique()\n",
    "print(f\"\\nChosen k: {chosen_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608eb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIT FINAL CLUSTERING\n",
    "# ============================================\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=chosen_k, random_state=42, n_init=10)\n",
    "y_km = kmeans_final.fit_predict(X)\n",
    "\n",
    "silhouette_final = silhouette_score(X, y_km)\n",
    "print(f\"Final Silhouette Score: {silhouette_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6aed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REMAP LABELS TO GOLD STANDARD\n",
    "# ============================================\n",
    "\n",
    "mapping = {}\n",
    "for cluster_id in np.unique(y_km):\n",
    "    mask = y_km == cluster_id\n",
    "    most_frequent = mode(y[mask], keepdims=True).mode[0]\n",
    "    mapping[cluster_id] = most_frequent\n",
    "    print(f\"Cluster {cluster_id} -> Label {most_frequent}\")\n",
    "\n",
    "y_km_remapped = np.array([mapping[label] for label in y_km])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0750234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFUSION MATRIX\n",
    "# ============================================\n",
    "\n",
    "cm = confusion_matrix(y, y_km_remapped)\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "print(f\"Clustering Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - Clustering\\nAccuracy: {accuracy:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPROCESSING: SCALE AND REFIT\n",
    "# ============================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans_scaled = KMeans(n_clusters=chosen_k, random_state=42, n_init=10)\n",
    "y_km_scaled = kmeans_scaled.fit_predict(X_scaled)\n",
    "silhouette_scaled = silhouette_score(X_scaled, y_km_scaled)\n",
    "\n",
    "print(f\"Silhouette (scaled): {silhouette_scaled:.4f}\")\n",
    "print(f\"Improvement: {silhouette_scaled - silhouette_final:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17d2d",
   "metadata": {},
   "source": [
    "## Section 8.5: Preprocessing for Clustering\n",
    "\n",
    "**üìå Use for: Clustering only**\n",
    "\n",
    "**Critical: Clustering algorithms are sensitive to feature scales!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026780b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WHY PREPROCESSING MATTERS FOR CLUSTERING\n",
    "# ============================================\n",
    "\n",
    "# Distance-based algorithms (K-Means, DBSCAN, Hierarchical) are affected by:\n",
    "# 1. Feature scales: A feature with range [0, 1000] dominates one with [0, 1]\n",
    "# 2. Units: km vs meters, dollars vs cents\n",
    "# 3. Categorical features: Need encoding before clustering\n",
    "\n",
    "print(\"‚ö†Ô∏è CRITICAL for clustering:\")\n",
    "print(\"   - Features with larger scales dominate distance calculations\")\n",
    "print(\"   - ALWAYS scale numeric features before clustering\")\n",
    "print(\"   - Encode categorical features appropriately\")\n",
    "print()\n",
    "\n",
    "# Show example of scale impact\n",
    "print(\"Example: Without scaling\")\n",
    "print(\"  Feature1: [1, 2, 3] (small range)\")\n",
    "print(\"  Feature2: [100, 200, 300] (large range)\")\n",
    "print(\"  ‚Üí Distance is dominated by Feature2!\")\n",
    "print()\n",
    "print(\"After scaling:\")\n",
    "print(\"  Feature1: [0, 0.5, 1]\")\n",
    "print(\"  Feature2: [0, 0.5, 1]\")\n",
    "print(\"  ‚Üí Both features contribute equally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43761e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MINMAXSCALER VS STANDARDSCALER\n",
    "# ============================================\n",
    "\n",
    "# MinMaxScaler: Scales to [0, 1] range\n",
    "#   - Good when: You want bounded range\n",
    "#   - Formula: (x - min) / (max - min)\n",
    "\n",
    "# StandardScaler: Scales to mean=0, std=1\n",
    "#   - Good when: Features follow normal distribution or have outliers\n",
    "#   - Formula: (x - mean) / std\n",
    "\n",
    "# Example with sample data\n",
    "sample_data = np.array([[1, 100], [2, 200], [3, 300], [4, 400], [5, 500]])\n",
    "\n",
    "# MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "data_minmax = minmax.fit_transform(sample_data)\n",
    "\n",
    "# StandardScaler\n",
    "standard = StandardScaler()\n",
    "data_standard = standard.fit_transform(sample_data)\n",
    "\n",
    "print(\"Original data (2 features with different scales):\")\n",
    "print(sample_data)\n",
    "print(f\"\\nMinMaxScaler result (range [0, 1]):\")\n",
    "print(data_minmax)\n",
    "print(f\"\\nStandardScaler result (mean=0, std=1):\")\n",
    "print(data_standard)\n",
    "print(\"\\nüí° For clustering exams, MinMaxScaler is often sufficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74259668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HANDLING CATEGORICAL FEATURES\n",
    "# ============================================\n",
    "\n",
    "# If your clustering data has categorical features, encode them BEFORE clustering\n",
    "\n",
    "# Option 1: OneHotEncoder (for nominal categories)\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "# categorical_encoded = encoder.fit_transform(df[['category_col']])\n",
    "\n",
    "# Option 2: OrdinalEncoder (if categories have order)\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# ordinal_encoded = ordinal_encoder.fit_transform(df[['education']])\n",
    "\n",
    "# Then combine with numeric features and scale everything\n",
    "\n",
    "print(\"üí° Categorical features must be encoded numerically before clustering\")\n",
    "print(\"   - Nominal (no order): OneHotEncoder\")\n",
    "print(\"   - Ordinal (has order): OrdinalEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854fb543",
   "metadata": {},
   "source": [
    "## Section 9: REGRESSION - Feature Selection by Correlation\n",
    "\n",
    "**Use this section for regression problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af371e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARE DATA FOR REGRESSION\n",
    "# ============================================\n",
    "\n",
    "# Target variable (adjust name)\n",
    "target_col = 'y'  # CHANGE THIS\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Check correlations\n",
    "correlation_matrix = df.corr()\n",
    "target_corr = correlation_matrix[target_col].sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation with target:\")\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce654c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IDENTIFY LOW CORRELATION FEATURES\n",
    "# ============================================\n",
    "\n",
    "threshold = 0.15  # Adjust based on exam requirements\n",
    "low_corr_features = target_corr[abs(target_corr) < threshold].index.tolist()\n",
    "\n",
    "if target_col in low_corr_features:\n",
    "    low_corr_features.remove(target_col)\n",
    "\n",
    "print(f\"\\nFeatures with |correlation| < {threshold}:\")\n",
    "print(low_corr_features)\n",
    "\n",
    "# Create reduced dataset\n",
    "X_reduced = X.drop(columns=low_corr_features)\n",
    "print(f\"\\nOriginal features: {X.shape[1]}\")\n",
    "print(f\"Reduced features: {X_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb52ac",
   "metadata": {},
   "source": [
    "## Section 10: REGRESSION - Models and Evaluation\n",
    "\n",
    "**Train and compare regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15209d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SPLIT DATA\n",
    "# ============================================\n",
    "\n",
    "# Full dataset\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reduced dataset\n",
    "X_train_reduced, X_test_reduced, y_train_r, y_test_r = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set (full): {X_train_full.shape}\")\n",
    "print(f\"Training set (reduced): {X_train_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LINEAR REGRESSION - FULL DATASET\n",
    "# ============================================\n",
    "\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_train_full, y_train)\n",
    "y_pred_full = lr_full.predict(X_test_full)\n",
    "rmse_full = np.sqrt(mean_squared_error(y_test, y_pred_full))\n",
    "\n",
    "print(f\"Linear Regression (Full Dataset)\")\n",
    "print(f\"RMSE: {rmse_full:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LINEAR REGRESSION - REDUCED DATASET\n",
    "# ============================================\n",
    "\n",
    "lr_reduced = LinearRegression()\n",
    "lr_reduced.fit(X_train_reduced, y_train_r)\n",
    "y_pred_reduced = lr_reduced.predict(X_test_reduced)\n",
    "rmse_reduced = np.sqrt(mean_squared_error(y_test_r, y_pred_reduced))\n",
    "\n",
    "print(f\"Linear Regression (Reduced Dataset)\")\n",
    "print(f\"RMSE: {rmse_reduced:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DECISION TREE REGRESSOR - REDUCED DATASET\n",
    "# ============================================\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train_reduced, y_train_r)\n",
    "y_pred_dt = dt_reg.predict(X_test_reduced)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test_r, y_pred_dt))\n",
    "\n",
    "print(f\"Decision Tree Regressor (Reduced Dataset)\")\n",
    "print(f\"RMSE: {rmse_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIMIZE DECISION TREE DEPTH\n",
    "# ============================================\n",
    "\n",
    "param_grid = {'max_depth': list(range(1, 21)) + [None]}\n",
    "\n",
    "grid_reg = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_reg.fit(X_train_reduced, y_train_r)\n",
    "\n",
    "best_dt_reg = grid_reg.best_estimator_\n",
    "best_depth = grid_reg.best_params_['max_depth']\n",
    "\n",
    "print(f\"Best max_depth: {best_depth}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_reg.best_score_):.4f}\")\n",
    "\n",
    "# Test\n",
    "y_pred_best = best_dt_reg.predict(X_test_reduced)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test_r, y_pred_best))\n",
    "print(f\"Test RMSE: {rmse_best:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE REGRESSION MODELS\n",
    "# ============================================\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Linear Reg (Full)',\n",
    "        'Linear Reg (Reduced)',\n",
    "        'Decision Tree (Reduced)',\n",
    "        'Decision Tree Optimized'\n",
    "    ],\n",
    "    'RMSE': [rmse_full, rmse_reduced, rmse_dt, rmse_best],\n",
    "    'Features': [X.shape[1], X_reduced.shape[1], X_reduced.shape[1], X_reduced.shape[1]]\n",
    "})\n",
    "\n",
    "print(\"\\nRegression Model Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4ba91",
   "metadata": {},
   "source": [
    "## Section 11: ASSOCIATION RULES - Data Cleaning\n",
    "\n",
    "**Use this section for transactional data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d01a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD TRANSACTIONAL DATA\n",
    "# ============================================\n",
    "\n",
    "# df = pd.read_excel(\"Online-Retail-France.xlsx\")\n",
    "\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CLEAN DESCRIPTIONS\n",
    "# ============================================\n",
    "\n",
    "print(f\"Unique descriptions before: {df['Description'].nunique()}\")\n",
    "\n",
    "# Strip whitespace\n",
    "df['Description'] = df['Description'].str.strip()\n",
    "\n",
    "print(f\"Unique descriptions after: {df['Description'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79403d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REMOVE INVALID TRANSACTIONS\n",
    "# ============================================\n",
    "\n",
    "# Remove rows without InvoiceNo\n",
    "print(f\"Rows before: {df.shape[0]}\")\n",
    "df = df.dropna(subset=['InvoiceNo'])\n",
    "print(f\"After removing NaN InvoiceNo: {df.shape[0]}\")\n",
    "\n",
    "# Remove credit transactions (starting with 'C')\n",
    "df = df[~df['InvoiceNo'].astype(str).str.contains('C')]\n",
    "print(f\"After removing credit transactions: {df.shape[0]}\")\n",
    "\n",
    "# Remove POSTAGE\n",
    "df = df[~df['Description'].str.contains('POSTAGE', na=False)]\n",
    "print(f\"After removing POSTAGE: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c729bb7",
   "metadata": {},
   "source": [
    "## Section 12: ASSOCIATION RULES - Basket Matrix\n",
    "\n",
    "**Create one-hot encoded basket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88362e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE BASKET MATRIX\n",
    "# ============================================\n",
    "\n",
    "basket = (\n",
    "    df.groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "    .sum()\n",
    "    .unstack()\n",
    "    .reset_index()\n",
    "    .fillna(0)\n",
    "    .set_index('InvoiceNo')\n",
    ")\n",
    "\n",
    "print(f\"Basket shape: {basket.shape}\")\n",
    "print(f\"Transactions: {basket.shape[0]}\")\n",
    "print(f\"Items: {basket.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVERT TO BOOLEAN\n",
    "# ============================================\n",
    "\n",
    "def encode(x):\n",
    "    return x > 0\n",
    "\n",
    "basket_bool = basket.map(encode)\n",
    "print(f\"Boolean basket shape: {basket_bool.shape}\")\n",
    "basket_bool.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5250e",
   "metadata": {},
   "source": [
    "## Section 13: ASSOCIATION RULES - Apriori\n",
    "\n",
    "**Find optimal support and generate rules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a02d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIND OPTIMAL MIN_SUPPORT\n",
    "# ============================================\n",
    "\n",
    "min_support = 1.0\n",
    "target_rules = 20  # Adjust based on exam\n",
    "rules = pd.DataFrame()\n",
    "\n",
    "print(\"Searching for optimal min_support...\\n\")\n",
    "\n",
    "while min_support > 0:\n",
    "    frequent_itemsets = apriori(\n",
    "        basket_bool,\n",
    "        min_support=min_support,\n",
    "        use_colnames=True\n",
    "    )\n",
    "    \n",
    "    if len(frequent_itemsets) > 0:\n",
    "        rules = association_rules(\n",
    "            frequent_itemsets,\n",
    "            metric='lift',\n",
    "            min_threshold=1\n",
    "        )\n",
    "    \n",
    "    print(f\"min_support={min_support:.2f}: {len(rules)} rules\")\n",
    "    \n",
    "    if len(rules) >= target_rules:\n",
    "        break\n",
    "    \n",
    "    min_support -= 0.01\n",
    "\n",
    "print(f\"\\nOptimal min_support: {min_support:.2f}\")\n",
    "print(f\"Rules generated: {len(rules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3464c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SORT AND DISPLAY RULES\n",
    "# ============================================\n",
    "\n",
    "rules_sorted = rules.sort_values(by=['lift', 'confidence'], ascending=False)\n",
    "\n",
    "print(\"Top 10 Association Rules:\")\n",
    "print(\"=\"*80)\n",
    "print(rules_sorted[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE RULES\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(rules_sorted['confidence'], rules_sorted['lift'], alpha=0.6, s=100)\n",
    "plt.xlabel('Confidence', fontsize=12)\n",
    "plt.ylabel('Lift', fontsize=12)\n",
    "plt.title('Association Rules: Confidence vs Lift', fontsize=14)\n",
    "plt.axhline(y=1, color='r', linestyle='--', label='Lift = 1')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a4a5b",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "**How to Use This Starter Kit:**\n",
    "\n",
    "1. **Identify your exam type** (Classification, Clustering, Regression, Association Rules)\n",
    "2. **Run Section 1 and 2** (imports and data loading) - always needed\n",
    "3. **Choose relevant sections**:\n",
    "   - Classification: Sections 3, 4, 5, 6\n",
    "   - Clustering: Sections 3, 7, 8\n",
    "   - Regression: Sections 3, 4, 5, 9, 10\n",
    "   - Association Rules: Sections 11, 12, 13\n",
    "4. **Adjust parameters** (file names, column names, thresholds)\n",
    "5. **Remove unused sections** for cleaner submission\n",
    "\n",
    "**Key Tips:**\n",
    "- Always check column names and adjust variables accordingly\n",
    "- Read exam requirements carefully for scoring metric (recall_macro, accuracy, etc.)\n",
    "- Comment your code with references to exam requirements\n",
    "- Remove test/debug code before submission\n",
    "- Use uniform variable naming in English\n",
    "\n",
    "**Good Luck! üçÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mldm)",
   "language": "python",
   "name": "mldm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
